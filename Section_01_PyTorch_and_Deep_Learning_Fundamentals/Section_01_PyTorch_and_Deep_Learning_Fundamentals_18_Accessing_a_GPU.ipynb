{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accessing a GPU"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deoarece există situații în care dorim să reproducem o anumită rețea neuronală, este necesar să putem să creem tensori 'random' pe care putem să îi reproducem. Aici intră în acțiune conceptul de **random seed**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating 2 random tensors without the random seed and comparing them\n",
      "tensor([0.3904, 0.6009, 0.2566, 0.7936])\n",
      "tensor([0.9408, 0.1332, 0.9346, 0.5936])\n",
      "tensor([False, False, False, False])\n",
      "\n",
      "Creating 2 random tensors with random seed and comparing them\n",
      "tensor([0.8823, 0.9150, 0.3829, 0.9593])\n",
      "tensor([0.8823, 0.9150, 0.3829, 0.9593])\n",
      "tensor([True, True, True, True])\n"
     ]
    }
   ],
   "source": [
    "# importing the library\n",
    "import torch\n",
    "\n",
    "# creating 2 random tensors without the random seed\n",
    "print('Creating 2 random tensors without the random seed and comparing them')\n",
    "tensor1 = torch.rand(4)\n",
    "tensor2 = torch.rand(4)\n",
    "print(tensor1)\n",
    "print(tensor2)\n",
    "print(tensor1 == tensor2)\n",
    "\n",
    "# creating the constant for random seed\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# setting the random seed in PyTorch\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "# creating 2 random tensors with random seed\n",
    "print('\\nCreating 2 random tensors with random seed and comparing them')\n",
    "tensor3 = torch.rand(4)\n",
    "\n",
    "# we need to set again the seed after using 'torch.rand()'\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "tensor4 = torch.rand(4)\n",
    "print(tensor3)\n",
    "print(tensor4)\n",
    "print(tensor3 == tensor4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La începutul acestui curs spuneam că o rețea neuronală poate fi alcătuită din o mulțime de tensori, iar între acești tensori există numeroase calcule, numeroase computații. Aceste calcule sunt mult mai rapide pe un GPU (placă grafică) deoarece acestea sunt special concepute pentru a realiza calcule numerice extrem de rapid. Din acest motiv, PyTorch ne permite să rulăm cod pe un `GPU` pentru a face un improvement de speed la funcționalitatea codului."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ca să verificăm dacă avem la dispoziție o placă grafică pe care am putea să o folosim o să ne folosim de 'cuda'. Comanda care trebuie folosită este `torch.cuda.isavailable()`. Comanda de mai sus ne returnează True dacă avem un GPU disponibil pe care să îl folosim și False dacă envoironment-ul unde rulăm codul nu ne pune la dispoziție un GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nu este destul să facem doar asta pentru a ne rula codul pe un GPU. Mai există ceva ce poartă denumirea de `device agnostic code`. Este un concept destul de important în Pytorch deoarece depinzând unde anume o să rulăm codul de Python este posibil să avem la dispoziție sau să nu avem la dispoziție un GPU pe care să rulăm acest cod. Aici putem să punem condiția dacă avem un GPU atunci să îl folosim, dacă nu, atunci să folosim un CPU (un procesor)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modul prin care facem asta este să ne creem o variabilă (prin convenție, această variabilă poartă denumirea de **device**). Momentan am stabilit-o la 'cuda', dare înseamnă placă grafică (GPU). Trebuie să verificăm dacă avem la dispoziție o placă grafică și doar atunci să rulăm codul pe placa grafică, altfel o să rulăm codul pe procesoare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De asemenea putem să verificăm și câte plăci grafice avem disponibile (există situații unde lucrăm cu componente din cloud și avem la dispoziție mai multe componente grafice), fiecare dintre placa putând să fie folosită pentru a rula un anumit cod specific. Pentru asta o să folosim `torch.cuda.device_count()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numărul returnat reprezintă numărul de plăci grafice de care dipsunem și de care ne putem folosi."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recapitulare"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "În cadrul acestui curs am învățat\n",
    "\n",
    "1. Cum să verificăm dacă dispunem de un GPU pe care să rulăm un cod de PyTorch\n",
    "\n",
    "```python\n",
    "import torch\n",
    "\n",
    "torch.cuda.is_available()\n",
    "```\n",
    "\n",
    "2. Cum să stabilim device-ul pe care să ruleze cod-ul de PyTorch în funcție de disponibilitatea plăcii grafice\n",
    "\n",
    "```python\n",
    "import torch\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available else 'cpu'\n",
    "```\n",
    "\n",
    "3. Cum să verificăm numărul de GPU-uri pe care le avem la dispoziție\n",
    "\n",
    "```python\n",
    "import torch\n",
    "\n",
    "torch.cuda.device_count()\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bf6313d899124d6a156763bc425ebfab90fa8eebf7ed3e373882edbe825970ae"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
